<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_otbh4bg885fo-3>li{counter-increment:lst-ctn-kix_otbh4bg885fo-3}.lst-kix_jtyejslgozxl-0>li{counter-increment:lst-ctn-kix_jtyejslgozxl-0}ol.lst-kix_otbh4bg885fo-4.start{counter-reset:lst-ctn-kix_otbh4bg885fo-4 0}.lst-kix_jtyejslgozxl-5>li{counter-increment:lst-ctn-kix_jtyejslgozxl-5}ol.lst-kix_otbh4bg885fo-0.start{counter-reset:lst-ctn-kix_otbh4bg885fo-0 0}ol.lst-kix_jtyejslgozxl-3.start{counter-reset:lst-ctn-kix_jtyejslgozxl-3 0}ol.lst-kix_jtyejslgozxl-6.start{counter-reset:lst-ctn-kix_jtyejslgozxl-6 0}.lst-kix_jtyejslgozxl-6>li{counter-increment:lst-ctn-kix_jtyejslgozxl-6}.lst-kix_iyk6bmexb7fr-2>li:before{content:"-  "}ol.lst-kix_otbh4bg885fo-6{list-style-type:none}ol.lst-kix_otbh4bg885fo-5{list-style-type:none}.lst-kix_iyk6bmexb7fr-1>li:before{content:"-  "}.lst-kix_iyk6bmexb7fr-3>li:before{content:"-  "}ol.lst-kix_otbh4bg885fo-4{list-style-type:none}ol.lst-kix_otbh4bg885fo-3{list-style-type:none}.lst-kix_jtyejslgozxl-6>li:before{content:"" counter(lst-ctn-kix_jtyejslgozxl-6,decimal) ". "}.lst-kix_jtyejslgozxl-8>li:before{content:"" counter(lst-ctn-kix_jtyejslgozxl-8,lower-roman) ". "}ol.lst-kix_otbh4bg885fo-2{list-style-type:none}ol.lst-kix_otbh4bg885fo-1{list-style-type:none}ol.lst-kix_otbh4bg885fo-3.start{counter-reset:lst-ctn-kix_otbh4bg885fo-3 0}ol.lst-kix_otbh4bg885fo-0{list-style-type:none}ol.lst-kix_otbh4bg885fo-1.start{counter-reset:lst-ctn-kix_otbh4bg885fo-1 0}.lst-kix_jtyejslgozxl-5>li:before{content:"" counter(lst-ctn-kix_jtyejslgozxl-5,lower-roman) ". "}.lst-kix_iyk6bmexb7fr-6>li:before{content:"-  "}.lst-kix_jtyejslgozxl-4>li:before{content:"" counter(lst-ctn-kix_jtyejslgozxl-4,lower-latin) ". "}.lst-kix_iyk6bmexb7fr-7>li:before{content:"-  "}ol.lst-kix_jtyejslgozxl-0.start{counter-reset:lst-ctn-kix_jtyejslgozxl-0 0}.lst-kix_iyk6bmexb7fr-0>li:before{content:"-  "}.lst-kix_iyk6bmexb7fr-8>li:before{content:"-  "}.lst-kix_jtyejslgozxl-3>li:before{content:"" counter(lst-ctn-kix_jtyejslgozxl-3,decimal) ". "}.lst-kix_otbh4bg885fo-1>li{counter-increment:lst-ctn-kix_otbh4bg885fo-1}.lst-kix_otbh4bg885fo-7>li{counter-increment:lst-ctn-kix_otbh4bg885fo-7}ol.lst-kix_jtyejslgozxl-7.start{counter-reset:lst-ctn-kix_jtyejslgozxl-7 0}.lst-kix_jtyejslgozxl-8>li{counter-increment:lst-ctn-kix_jtyejslgozxl-8}.lst-kix_iyk6bmexb7fr-5>li:before{content:"-  "}ol.lst-kix_otbh4bg885fo-7.start{counter-reset:lst-ctn-kix_otbh4bg885fo-7 0}.lst-kix_iyk6bmexb7fr-4>li:before{content:"-  "}.lst-kix_otbh4bg885fo-4>li{counter-increment:lst-ctn-kix_otbh4bg885fo-4}ol.lst-kix_otbh4bg885fo-8{list-style-type:none}ol.lst-kix_otbh4bg885fo-7{list-style-type:none}.lst-kix_jtyejslgozxl-7>li:before{content:"" counter(lst-ctn-kix_jtyejslgozxl-7,lower-latin) ". "}ul.lst-kix_iyk6bmexb7fr-0{list-style-type:none}ol.lst-kix_otbh4bg885fo-2.start{counter-reset:lst-ctn-kix_otbh4bg885fo-2 0}.lst-kix_otbh4bg885fo-6>li{counter-increment:lst-ctn-kix_otbh4bg885fo-6}ul.lst-kix_iyk6bmexb7fr-2{list-style-type:none}ul.lst-kix_iyk6bmexb7fr-1{list-style-type:none}ul.lst-kix_iyk6bmexb7fr-4{list-style-type:none}ul.lst-kix_iyk6bmexb7fr-3{list-style-type:none}ul.lst-kix_iyk6bmexb7fr-6{list-style-type:none}ul.lst-kix_iyk6bmexb7fr-5{list-style-type:none}.lst-kix_otbh4bg885fo-0>li:before{content:"" counter(lst-ctn-kix_otbh4bg885fo-0,decimal) ". "}.lst-kix_otbh4bg885fo-0>li{counter-increment:lst-ctn-kix_otbh4bg885fo-0}.lst-kix_otbh4bg885fo-1>li:before{content:"" counter(lst-ctn-kix_otbh4bg885fo-1,lower-latin) ". "}.lst-kix_jtyejslgozxl-2>li{counter-increment:lst-ctn-kix_jtyejslgozxl-2}ol.lst-kix_jtyejslgozxl-4.start{counter-reset:lst-ctn-kix_jtyejslgozxl-4 0}.lst-kix_otbh4bg885fo-3>li:before{content:"" counter(lst-ctn-kix_otbh4bg885fo-3,decimal) ". "}.lst-kix_jtyejslgozxl-0>li:before{content:"" counter(lst-ctn-kix_jtyejslgozxl-0,decimal) ". "}ol.lst-kix_jtyejslgozxl-8.start{counter-reset:lst-ctn-kix_jtyejslgozxl-8 0}.lst-kix_otbh4bg885fo-2>li:before{content:"" counter(lst-ctn-kix_otbh4bg885fo-2,lower-roman) ". "}.lst-kix_otbh4bg885fo-4>li:before{content:"" counter(lst-ctn-kix_otbh4bg885fo-4,lower-latin) ". "}.lst-kix_jtyejslgozxl-2>li:before{content:"" counter(lst-ctn-kix_jtyejslgozxl-2,lower-roman) ". "}ol.lst-kix_jtyejslgozxl-1.start{counter-reset:lst-ctn-kix_jtyejslgozxl-1 0}.lst-kix_jtyejslgozxl-1>li:before{content:"" counter(lst-ctn-kix_jtyejslgozxl-1,lower-latin) ". "}.lst-kix_otbh4bg885fo-6>li:before{content:"" counter(lst-ctn-kix_otbh4bg885fo-6,decimal) ". "}ol.lst-kix_otbh4bg885fo-6.start{counter-reset:lst-ctn-kix_otbh4bg885fo-6 0}.lst-kix_jtyejslgozxl-3>li{counter-increment:lst-ctn-kix_jtyejslgozxl-3}.lst-kix_otbh4bg885fo-5>li:before{content:"" counter(lst-ctn-kix_otbh4bg885fo-5,lower-roman) ". "}ol.lst-kix_jtyejslgozxl-5.start{counter-reset:lst-ctn-kix_jtyejslgozxl-5 0}ol.lst-kix_otbh4bg885fo-8.start{counter-reset:lst-ctn-kix_otbh4bg885fo-8 0}.lst-kix_jtyejslgozxl-1>li{counter-increment:lst-ctn-kix_jtyejslgozxl-1}.lst-kix_jtyejslgozxl-7>li{counter-increment:lst-ctn-kix_jtyejslgozxl-7}.lst-kix_otbh4bg885fo-7>li:before{content:"" counter(lst-ctn-kix_otbh4bg885fo-7,lower-latin) ". "}.lst-kix_jtyejslgozxl-4>li{counter-increment:lst-ctn-kix_jtyejslgozxl-4}.lst-kix_otbh4bg885fo-2>li{counter-increment:lst-ctn-kix_otbh4bg885fo-2}.lst-kix_otbh4bg885fo-5>li{counter-increment:lst-ctn-kix_otbh4bg885fo-5}.lst-kix_otbh4bg885fo-8>li:before{content:"" counter(lst-ctn-kix_otbh4bg885fo-8,lower-roman) ". "}.lst-kix_otbh4bg885fo-8>li{counter-increment:lst-ctn-kix_otbh4bg885fo-8}ol.lst-kix_jtyejslgozxl-2.start{counter-reset:lst-ctn-kix_jtyejslgozxl-2 0}ol.lst-kix_jtyejslgozxl-3{list-style-type:none}ol.lst-kix_jtyejslgozxl-2{list-style-type:none}ol.lst-kix_jtyejslgozxl-5{list-style-type:none}ol.lst-kix_otbh4bg885fo-5.start{counter-reset:lst-ctn-kix_otbh4bg885fo-5 0}ol.lst-kix_jtyejslgozxl-4{list-style-type:none}ol.lst-kix_jtyejslgozxl-1{list-style-type:none}ol.lst-kix_jtyejslgozxl-0{list-style-type:none}ul.lst-kix_iyk6bmexb7fr-8{list-style-type:none}ul.lst-kix_iyk6bmexb7fr-7{list-style-type:none}ol.lst-kix_jtyejslgozxl-7{list-style-type:none}ol.lst-kix_jtyejslgozxl-6{list-style-type:none}ol.lst-kix_jtyejslgozxl-8{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c3{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c14{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center;height:11pt}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Times New Roman";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c17{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c15{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:italic}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left;height:11pt}.c4{font-size:10pt;font-family:"Times New Roman";font-style:italic;font-weight:400}.c19{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c8{font-size:10pt;font-family:"Times New Roman";font-weight:400}.c18{font-weight:700;font-family:"Times New Roman"}.c10{margin-left:22.5pt;text-indent:-22.5pt}.c12{font-weight:400;font-family:"Times New Roman"}.c21{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c13{text-indent:36pt}.c20{font-style:italic}.c16{background-color:#ffffff}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c16 c21"><p class="c17"><span class="c14">STAT 419 Final Project: Neural Networks</span></p><p class="c17"><span class="c12">Daniel DeFoe</span><span class="c2">, Markelle Kelly, Sadie Rhen</span></p><p class="c5"><span class="c2"></span></p><p class="c7"><span class="c18">ABSTRACT.</span></p><p class="c7 c13"><span class="c2">Currently the world of machine learning gives statisticians and data scientists a plethora of tools and models to choose from when it comes to analyzing data and making predictions. With all of these choices at the disposal of professionals, neural networks have emerged as a powerful tool for data classification and outcome prediction. Modeling with neural networks has significantly advanced our power to make sense of large datasets and draw meaningful, increasingly accurate conclusions. Neural network architecture is behind the advancement of modern technological breakthroughs such as image and audio recognition. </span></p><p class="c7 c13"><span class="c2">The following report aims to describe and portray how a neural network can classify image data. This will be done using Python&rsquo;s Keras package with tensorflow. The general structure of the network will be discussed, including its layers, in terms of our data set. The produced model will then be broken down through evaluation of our findings and reflection upon our results. </span></p><hr><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c18">1. INTRODUCTION</span></p><p class="c7"><span class="c2">The data used to investigate how to build and develop a neural network are images of articles of clothing from the popular &ldquo;Fashion-MNIST&rdquo; dataset, obtained from Zalando Research on GitHub. Each observation is a 28x28 pixel black and white image classified as one of 10 possible types of clothing articles. Each image is classified as a t-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, or ankle boot. To describe each image, the neural network considers its pixels, counted horizontally row by row. This means that pixel 29 is in the 2nd row, in the 0th column. There are 784 total columns representing the pixels by their darkness, measured on a scale of 1 to 255. There are a total of 70,000 observations (images) in the dataset. </span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c15 c12">Why Neural Networks?</span></p><p class="c7"><span class="c12">Convolutional neural networks are one of the most effective strategies for image and pattern recognition and classification. Their convolutional layers, which will be discussed in detail in the next section, allow them to reliably identify patterns and features in images. Additionally, the way neural networks consider different variables and their relationships make them an ideal way to deal with sets of variables that are related in specific ways, like the sets of pixels comprising images.</span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c3">2. NEURAL NETWORK CREATION</span></p><p class="c7"><span class="c12 c15">Neural Network Layers</span></p><p class="c7"><span class="c12">The Keras package for Python used to create the network employs a sequential model, so it allows the model to be defined and built layer by layer.</span><span class="c12 c20">&nbsp;</span><span class="c2">The input layer to the network consists of the data&rsquo;s features as described in the introduction. Collectively the data set&rsquo;s variables serve as predictors, with each pixel assuming the role of a single input node, for a total of 784 nodes. </span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c12">The hidden layers of the network include different types of layers with various functions that essentially comprise the entire model. It is important to note that the creation of the series of hidden layers is largely synonymous with the creation of the model. The number and nature of these layers plays a significant role in the model&rsquo;s ability to correctly classify data. The model used for the Fashion-MNIST data was a two dimensional convolutional neural network, so the first layer was a convolutional layer. This layer uses a filter referred to as a kernel to go through the input image, one section at a time, grouping the pixels into receptive fields. For each position on the image, weights assigned to the previously determined groups of pixels are multiplied by each pixel to essentially determine importance by area. </span><span class="c12 c16">Convolutional layer filters act as feature identifiers - the element wise multiplications will increase dramatically if there is a pattern in parts of the image that match the filter. </span><span class="c12">These element wise multiplications end up being represented in a single array of </span><span class="c12">numbers called a feature map. The more kernels there are, the better the network can capture differences between groups of pixels. After this first layer, we added another convolutional layer, then two dense layers. The dense layers conduct operations on the preceding layer, using the calculated weights to connect each node to each neuron in the next layer, further down the network, with updated values based on the weights.</span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c2">It should be noted that in the defining of the hidden layers, a max-pooling aggregation was applied as well as two dropout functions. Max-pooling helps prevent overfitting by aggregating the maximums of each feature group after the convolutional layers. The dropout functions also help prevent overfitting by setting a portion (for our model, 25%, then 50%) of the activations of a given layer to zero during each step of model training. For further detail on the exact model we used, see our Keras code in the appendix.</span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c12">Finally, the output layer, containing 10 nodes, contains the neural network&rsquo;s prediction of what the first column of the data, the image classification, should be. From the computing done in the series of hidden layers, the model makes a prediction for each observation of what kind of clothing article the image is. To represent this, one of the 10 nodes, each of which correspond to one of our 10 original categories, will have a value of 1, and the rest will have a value of 0 (the node with 1 representing the predicted category). We saved these predictions for further analysis of which categories our network was most frequently getting wrong (discussed in the next section).</span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c15 c12">Model Training</span></p><p class="c7"><span class="c2">We trained our neural network on 60,000 of the 70,000 total images, leaving 10,000 for testing the model.</span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c12">In order for the network to classify the data properly, the weights and biases of each layer need to be optimized. In order to do this, neural networks need a cost function and optimization algorithm. When a single piece of training data is run through the unoptimized model, the value of each output node is compared to what was expected, generally one for the category to which the piece of training data belongs and zero for all other categories. Popular cost functions include mean square error, cross entropy, and absolute error; we used a categorical cross entropy loss function, which determines the difference between the true and observed probability distributions</span><span class="c12">;</span><span class="c12">&nbsp;m</span><span class="c12">inimizing cross entropy maximizes the log likelihood.</span><span class="c12">&nbsp;Sin</span><span class="c2">ce this cost function will be small when the output nodes are close to their expected value, minimizing this cost function by adjusting the weights and bias of each layer of the network will reduce the overall error in the network. </span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c2">To perform this optimization, we used Adadelta, a gradient descent algorithm, which applies the negative gradient of the cost function to all of the weights and biases in a layer. This reveals how much each piece should be adjusted in order to decrease the cost function by the greatest amount. Then, this process is repeated for each layer, optimizing the entire network. </span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c2">Since the entire training data set of 60,000 points (one epoch) cannot be run through the network all at once, a cost function and corresponding gradient was computed for batches of 128 points at a time, and the network&rsquo;s weights and biases were optimized based on that. Batches were run until all 60,000 points had been used, and the first epoch was completed. Since only running through the training data once tends to underfit the data, multiple epochs need to be run in order to increase the accuracy of the network. However, running too many epochs can overfit the network to the specific training data, making it less accurate when classifying new data that is not from the training set. By 12 epochs, the increases in accuracy were smaller than .005 per step, and the network had an overall accuracy of .9406 for the training data, so training was considered to be complete. The specific performance of each epoch is included in the appendix. </span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c15 c12">Model Evaluation</span></p><p class="c7"><span class="c12">We then evaluated our model based on our test set of 10,0</span><span class="c12">00 additional images. Within the training, </span><span class="c12">we brought our loss down from 0.5684 to 0.1660, and increased our accuracy from .7978 after the first epoch to .9406. When we evaluated our neural network with the test data, it achieved a loss of .214 and an accuracy of .9267. </span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c3">3. FINDINGS</span></p><p class="c0"><span class="c3"></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 237.00px; height: 255.00px;"><img alt="" src="images/image1.png" style="width: 406.29px; height: 304.21px; margin-left: -73.36px; margin-top: -15.49px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c2">Figure 1 shows the general accuracy of the predictions provided by the network when run using a single epoch. This makes it easy to visualize which articles are incorrectly classified, and why that may be. The confusion matrix shows that the articles most commonly classified incorrectly are shirts, with only 58.6% of them correctly identified. However, this category makes sense as a problematic one; these shirts were most frequently mistaken for t-shirts/tops, pullovers, and coats. These are very similar articles of clothing, with very similar features (comparable shape, visual nature of having sleeves), and this makes it understandable that the network would have trouble consistently classifying them - even humans could have trouble distinguishing shirts from tops. (In this data set, shirts are nicer blouses, while tops are more casual). The articles with the highest proportion correctly classified were trousers and bags, each at 96.8% correct, which makes sense because each of these items has its own unique features. Trousers are the only pant item being classified and bags have a very distinct shape compared to the other items, which are all physically worn on the body. These distinctive visual features allow trousers and bags to be effectively classified by this somewhat limited neural network.</span></p><p class="c7"><span class="c2">Figure 2 is the same confusion matrix as Figure 1, but recreated after running all 12 epochs. The increased accuracy for every category shows the benefit of running multiple epochs. Going through the same training data multiple times significantly improved our accuracy on the test data. Shirts are still the most frequently misclassified, but now over 75% of them are correctly identified, a significant improvement from 58.6% after a single epoch. Our neural network is most accurate for trousers, sandals, sneakers, and bags.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 243.50px; height: 253.65px;"><img alt="" src="images/image4.png" style="width: 417.43px; height: 313.07px; margin-left: -75.37px; margin-top: -23.19px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c13"><span class="c2"></span></p><p class="c7"><span class="c2">Just by looking at the clear diagonal lines in both confusion matrices, one can conclude that our neural network is closely matching reality, and is a fairly reliable indicator of an image&rsquo;s true clothing classification.</span></p><p class="c0"><span class="c2"></span></p><p class="c7"><span class="c2">While near 93% accuracy is impressive for a relatively simple model, we could always work to improve our neural network by expanding the hidden layer. According to the dataset&rsquo;s GitHub page, other convolutional neural networks have achieved up to 95% accuracy. To improve our model, we would consider adding another convolutional or dense layer, or pre-processing the data. This page also lists benchmarks for other types of classifiers, including SVC, random forests, and K-neighbors, which all have accuracies in the mid 80 percents, so neural networks appear to be an optimal way to classify this dataset.</span></p><p class="c0 c13"><span class="c2"></span></p><p class="c7"><span class="c2">It is worth noting that in the modern age, there is often a heightened importance placed on the accuracy of neural network models when it comes to certain systems, especially in the field of image recognition. Artificial intelligence utilizing neural networks is becoming increasingly responsible for the maintenance of systems pertaining to safety and security. To cite a few examples, self driving cars interpreting the highly variable surrounding world require very high accuracy to be able to safely navigate the road, and facial recognition to unlock modern smartphones is often the only barrier between a potential predator and someone&rsquo;s private information. It is in systems such as this that networks must be built with much more extensive and powerful hidden layers to increase overall accuracy. It is imperative for the prediction of these networks to be reliable in order to protect users and coexisting entities. </span></p><p class="c0 c13"><span class="c2"></span></p><p class="c7"><span class="c3">4. REFERENCES </span></p><p class="c7 c10"><span class="c8">3Blue1Brown. &ldquo;But What *Is* a Neural Network? | Chapter 1, Deep Learning.&rdquo; </span><span class="c4">YouTube</span><span class="c6">, YouTube, 5 Oct. 2017, www.youtube.com/watch?v=aircAruvnKk.</span></p><p class="c7 c10"><span class="c8">3Blue1Brown. &ldquo;Gradient Descent, How Neural Networks Learn | Chapter 2, Deep Learning.&rdquo; </span><span class="c4">YouTube</span><span class="c8">, YouTube, 16 Oct. 2017, www.youtube.com/watch?v=IHZwWFHWa-w.</span></p><p class="c19 c10"><span class="c8">Clabaugh, Caroline, et al. &ldquo;Neural Networks - History.&rdquo; </span><span class="c4">Neural Networks</span><span class="c8">, 2000, cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html.</span></p><p class="c19 c10"><span class="c8">Izenman, Alan Julian. </span><span class="c4">Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning</span><span class="c6">. Springer, 2013.</span></p><p class="c19 c10"><span class="c8">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. Han Xiao, Kashif Rasul, Roland Vollgraf. </span><span class="c8">arXiv:1708.07747</span><span class="c6">.</span></p><p class="c10 c19"><span class="c8">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;github.com/zalandoresearch/fashion-mnist</span></p><p class="c19 c10"><span class="c8">Sharma, Sagar. &ldquo;Epoch vs Batch Size vs Iterations &ndash; Towards Data Science.&rdquo; </span><span class="c4">Towards Data Science</span><span class="c6">, 22 Sept. 2017, towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9.</span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c10 c11"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><p class="c11 c10"><span class="c1"></span></p><hr style="page-break-before:always;display:none;"><p class="c11 c10"><span class="c1"></span></p><p class="c7"><span class="c18">5</span><span class="c3">. APPENDIX </span></p><p class="c7"><span class="c2">Figure 3. Output for our neural network </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 610.50px; height: 277.15px;"><img alt="" src="images/image2.png" style="width: 610.50px; height: 277.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c2">Figure 4. Our model code in Keras</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 391.71px; height: 444.50px;"><img alt="" src="images/image3.png" style="width: 391.71px; height: 444.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span class="c2"></span></p><div><p class="c0"><span class="c9"></span></p></div></body></html>